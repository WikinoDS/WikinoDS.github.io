<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Vinicius Aquino">

<title>Gradient Boosting com XGBoost Aplicado à Séries Temporais</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="qmd_file/libs/clipboard/clipboard.min.js"></script>
<script src="qmd_file/libs/quarto-html/quarto.js"></script>
<script src="qmd_file/libs/quarto-html/popper.min.js"></script>
<script src="qmd_file/libs/quarto-html/tippy.umd.min.js"></script>
<script src="qmd_file/libs/quarto-html/anchor.min.js"></script>
<link href="qmd_file/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="qmd_file/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="qmd_file/libs/bootstrap/bootstrap.min.js"></script>
<link href="qmd_file/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="qmd_file/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Gradient Boosting com XGBoost Aplicado à Séries Temporais</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Vinicius Aquino </p>
          </div>
  </div>
    
  
    
  </div>
  

</header>

<section id="resumo" class="level2">
<h2 class="anchored" data-anchor-id="resumo">Resumo:</h2>
<p>
O presente texto é uma introdução aos modelos Gradient Boosting para séries temporais. Nele foram apresentados a classe de modelos de regressão não paramétrica Gradient Boosting Decision Tree (GBDT) e o XGBoost, uma subclasse dos modelos GBDT. Por fim, foi aplicado o XGBoost no contexto de séries temporais. O texto foi construído sob conceitos não tão difundidos no contexto de Machine Learning, como regressões não paramétricas, a fim de que o leitor possa generalizar para outros modelos, como KNN, LightGBM, Random Forest etc.
</p>
<p><br></p>
<p>Pacotes usados:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats <span class="im">as</span> st</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> calendar</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pmdarima.model_selection <span class="im">import</span> SlidingWindowForecastCV</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> xgboost <span class="im">import</span> XGBRegressor</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">'ggplot'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
</section>
<section id="a-definição-do-modelo" class="level2">
<h2 class="anchored" data-anchor-id="a-definição-do-modelo">A Definição do modelo:</h2>
<section id="regressão-não-paramétrica" class="level3">
<h3 class="anchored" data-anchor-id="regressão-não-paramétrica">Regressão Não Paramétrica</h3>
<p>
</p><p>Modelos da classe Gradient Boosting fazem parte de uma classe maior de modelos definida como <i>regressão não paramétrica</i>. Assim como modelos de regressão linear, essa é uma classe de modelo onde se usa <i>variáveis explanatórias X</i> para predizer o valor esperado de uma <i>variável explicada Y</i>.</p>
<p>A grande diferença é que a regressão linear vai estar mais interessada nos parâmetros que explicam as relações entre <i>X</i> e <i>y</i>, fazendo certas suposições, como normalidade, por exemplo. Já as regressões não paramétricas vão tentar apenas achar uma relação entre as variáveis com suposições mais fracas da distribuição dos parâmetros.</p>
<p>A ideia central é definir uma função <span class="math inline">\(r(x_i)\)</span>, tal que:</p>
<p><span class="math display">\[y_i = r(x_i) + \epsilon_{i}\]</span></p>
<p>No caso, já dando um spoiler, as funções <span class="math inline">\(r(x_i)\)</span> serão árvores de decisão. Evidentemente espera-se que <span class="math inline">\(E(\epsilon_{i}) = 0\)</span></p>
São exemplos de modelos de regressão Não linear: o algoritmo KNN, Random Forest, Suavização por Splines, Arvores de regressão etc.
<p></p>
</section>
<section id="gradient-boosting-decision-tree" class="level3">
<h3 class="anchored" data-anchor-id="gradient-boosting-decision-tree">Gradient Boosting Decision Tree:</h3>
<p>
</p><p>Nesta lógica de se ter um estimador para uma variável <i>y</i> a partir de covariáveis <i>X</i>, nasce os algoritmos Gradient Boosting Decision Tree (GBDT). Como o nome sugere, eles são baseados em árvores de decisão. Diferente do <i>Random Forest</i>, os algoritmos GBDT não usam as árvores de decisão em paralelo.</p>
<p>No Random Forest, se tem várias árvores distintas vindas de reamostragens aleatórias de um conjunto de dados, onde a previsão é a média de todas as árvores juntas. No caso dos algoritmos GBDT é usado o processo de “Bosting”, onde se treina sequencialmente novas árvores de decisão que corrigem os erros das anteriores.</p>
<p>Isso é, a primeira árvore é treinada com os dados. A segunda com os resíduos que essa primeira árvore deixou. A terceira com os resíduos da segunda e assim sucessivamente. É um processo <i>iterativo</i>.</p>
Um exemplo abaixo inspirado no livro <a href="https://www.amazon.com.br/M%C3%A3os-obra-aprendizado-scikit-learn-tensorflow/dp/8550803812/ref=asc_df_8550803812/?tag=googleshopp00-20&amp;linkCode=df0&amp;hvadid=379748659420&amp;hvpos=&amp;hvnetw=g&amp;hvrand=6197706445306642602&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1001634&amp;hvtargid=pla-812887614657&amp;psc=1">Mãos à Obra: Aprendizado de Máquina com Scikit-Learn &amp; TensorFlow</a>:
<p></p>
<ul>
<li>Gerando dados aleatórios</li>
</ul>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">633</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.uniform(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">100</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">**</span> <span class="dv">2</span> <span class="op">+</span> <span class="dv">5</span> <span class="op">+</span> np.random.normal(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">100</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>ax.scatter(x<span class="op">=</span>X, </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>           y<span class="op">=</span>y, </span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>           color<span class="op">=</span><span class="st">'k'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-4-output-1.png" width="641" height="411"></p>
</div>
</div>
<p><br></p>
<ul>
<li>Treinando três árvores de decisão sequencialmente:</li>
</ul>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> X.reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>tree_1 <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>y_1 <span class="op">=</span> y <span class="op">-</span> tree_1.predict(X<span class="op">=</span>X) <span class="co"># Residuo da primeira Arvore</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>tree_2 <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y_1)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>y_2 <span class="op">=</span> y_1 <span class="op">-</span> tree_2.predict(X<span class="op">=</span>X) <span class="co"># Residuo da segunda Arvore</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>tree_3 <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>).fit(X, y_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>
</p><p>Dessa forma, cada árvore se especializou em aprender a “consertar” os erros das arvores que vieram antes. Voltando a ideia de regressão não paramétrica, o modelo inicial dado é:</p>
<p><span class="math display">\[y_i = r_1(x_i) + \epsilon_{1i} = tree\_1(x_i) + \epsilon_{1i}\]</span></p>
<p>A árvore 2 é treinada com os erros <span class="math inline">\(\epsilon_1\)</span> da árvore 1:</p>
<p><span class="math display">\[(y_i-tree\_1(x_i)) = \epsilon_{1i} =  r_1(x_i) + \epsilon_2i =  tree\_2(x_i) + \epsilon_2i\]</span></p>
<p>E, por fim, a árvore 3 é com os erros da árvore 2, isso é:</p>
<p><span class="math display">\[(y_i-tree\_1(x_i)) - tree\_2(x_i) = \epsilon_2i =  r(x_i) + \epsilon_3i = tree\_3(x_i) + \epsilon_3i\]</span></p>
<p>A idéia intuitiva é que se adicione novas árvores conforme isso reduza os erros em teste, até que não seja possível mais entender os padrões em <span class="math inline">\(\epsilon_{q}\)</span>.</p>
<p>Como o modelo em questão parou em <span class="math inline">\(\epsilon_{3}\)</span>, isso é, assume-se que <span class="math inline">\(\epsilon_{3}\)</span> não tem padrão, é um ruído, pode-se reescrever a equação acima como:</p>
<p><span class="math display">\[y_i-tree\_1(x_i) - tree\_2(x_i) - tree\_3(x_i) = \epsilon_3i\]</span></p>
<p><span class="math display">\[y_i = tree\_1(x_i) + tree\_2(x_i) + tree\_3(x_i) + \epsilon_3i\]</span></p>
<p>Ou seja, a previsão da i-esima observação <span class="math inline">\(y_i\)</span> é a soma das previsões das árvores. E, então, do ponto de vista da regressão não paramétrica, o modelo é dado por:</p>
<p><span class="math display">\[y_i = r(x_i) + \epsilon_{i} = \sum_{1}^{n} {tree\_{i}(x_i)} + \epsilon_{ni}\]</span></p>
<p>Onde cada modelo <span class="math inline">\(tree_{j}\)</span> foi treinado iterativamente e não em paralelo.</p>
Em python:
<p></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>x_space <span class="op">=</span> np.arange(<span class="op">-</span><span class="dv">5</span>, <span class="dv">6</span>).reshape(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> <span class="bu">sum</span>(tree.predict(x_space) <span class="cf">for</span> tree <span class="kw">in</span> (tree_1, tree_2, tree_3))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Visualizando o modelo:</p>
<div class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-7-output-1.png" width="641" height="411"></p>
</div>
</div>
<p><br></p>
<p>
</p><p>Perceba: ninguém está preocupado se o erro padrão dos nós destas árvores tem distribuição normal, são não viesados, assintóticos etc. Por isso trata-se de uma regressão não paramétrica.</p>
<p>O game change a partir de agora será como se adcionam essas árvores, como elas são modeladas. E é aí que vai residir a diferença entre o XGBoost e o LightGBM. Vale um estudo nos hiper-parâmetros desses modelos, afim de entender como eles se comportam.</p>
A partir de agora se usará o XGBoost sem tunagem de hiper-parâmetros, dado que a idéia é apenas entender sua aplicação em séries temporais
<p></p>
</section>
</section>
<section id="modelando-xgboost-vs-regressão-linear-para-previsão-de-vendas-de-cerveja" class="level2">
<h2 class="anchored" data-anchor-id="modelando-xgboost-vs-regressão-linear-para-previsão-de-vendas-de-cerveja">Modelando: XGBoost vs regressão linear para Previsão de Vendas de Cerveja:</h2>
<p><br></p>
<section id="baixando-base-de-dados" class="level3">
<h3 class="anchored" data-anchor-id="baixando-base-de-dados">Baixando base de dados:</h3>
<p>Os dados da venda de cerveja no USA podem ser acessados diretamente deste link do github <a href="https://raw.githubusercontent.com/flo7up/relataly_data/main/alcohol_sales/BeerWineLiquor.csv">aqui</a>.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(path, parse_dates<span class="op">=</span>[<span class="st">'date'</span>], index_col<span class="op">=</span>[<span class="st">'date'</span>])</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df.tail())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            beer
date            
2018-08-01  4898
2018-09-01  4598
2018-10-01  4737
2018-11-01  5130
2018-12-01  6370</code></pre>
</div>
</div>
<p><br></p>
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-10-output-1.png" width="668" height="434"></p>
</div>
</div>
<p><br></p>
</section>
<section id="metodologia" class="level3">
<h3 class="anchored" data-anchor-id="metodologia">Metodologia:</h3>
<p>A idéia da análise é fazer um comparativo de desempenho conforme se aumenta a sofisticação do XGBoost. Serão feitas validações cruzadas onde o objetivo do modelo será prever a demanda total do ano posterior. Serão usados 72 meses para prever a demanda dos próximos 12, como pode ser visto abaixo:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>cv <span class="op">=</span> SlidingWindowForecastCV(step<span class="op">=</span><span class="dv">12</span>, </span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>                             h<span class="op">=</span><span class="dv">12</span>, </span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>                             window_size<span class="op">=</span><span class="dv">72</span>)</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>cv_generator <span class="op">=</span> cv.split(df)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(cv.split(df)))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>f, axs <span class="op">=</span> plt.subplots(nrows<span class="op">=</span>n,</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>                      ncols<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>                      sharex<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>                      figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">12</span>))</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>f.suptitle(<span class="ss">f"Cross-Validation Subsets (</span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> <span class="bu">next</span>(cv_generator)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    axs[i].plot(df.iloc[index[<span class="dv">0</span>]], <span class="st">'k'</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    axs[i].plot(df.iloc[index[<span class="dv">1</span>]], <span class="st">'r'</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    axs[i].get_yaxis().set_visible(<span class="va">False</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-11-output-1.png" width="757" height="1133"></p>
</div>
</div>
<p><br></p>
<p>Além disso, será feito uso de um <i>teste de hipótese binomial</i> para validar o vencedor. A ideia do teste é bem simples:</p>
<p>Supõe-se que os modelos têm a mesma performance. Se isso é verdade, então a probabilidade de um modelo ter uma acurácia maior que o outro é 50%, o mero acaso.</p>
<p>Dessa forma, espera-se que o desempenho de cada modelo tenha uma distribuição binomial de média <span class="math inline">\(0.5n\)</span> nos n testes. Se for observado um valor que faça sentido a luz dessa hipótese, então os modelos tem performance similar.</p>
<p>Do contrário, rejeita-se a hipótese.</p>
<p>Aqui, uma função em python que executa o teste e retorna um <i>print()</i> para melhor entendimento do usuário.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> test(a, b, alpha <span class="op">=</span> <span class="fl">.05</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="bu">len</span>(a)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    a_venceu <span class="op">=</span> (a<span class="op">&gt;</span>b).<span class="bu">sum</span>()</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    b_venceu <span class="op">=</span> (b<span class="op">&gt;</span>a).<span class="bu">sum</span>()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Testes: </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"A venceu: </span><span class="sc">{</span>a_venceu<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"B venceu: </span><span class="sc">{</span>b_venceu<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> a_venceu <span class="op">&gt;=</span> b_venceu:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        vencedor <span class="op">=</span> <span class="st">"A"</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> a_venceu</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        vencedor <span class="op">=</span> <span class="st">"B"</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> b_venceu</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    resultado <span class="op">=</span> st.binomtest(k<span class="op">=</span>k, n<span class="op">=</span>n, </span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>                             p<span class="op">=</span><span class="fl">.5</span>, alternative<span class="op">=</span><span class="st">'greater'</span>)</span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> resultado.pvalue <span class="op">&gt;</span> alpha:</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Não há evidências para dizer que o </span><span class="sc">{</span>vencedor<span class="sc">}</span><span class="ss"> é o melhor modelo"</span>)</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Há evidências para dizer que o </span><span class="sc">{</span>vencedor<span class="sc">}</span><span class="ss"> é o melhor modelo"</span>)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"p valor: "</span>, <span class="bu">round</span>(resultado.pvalue, <span class="dv">3</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><br></p>
<p>
Esse teste supõe que as amostras são independentes, o que não é bem verdade, uma vez que o mesmo ano será usado até 6x para treino. De toda forma, será um bom norte.
</p>
<p><br></p>
</section>
<section id="modelando-a-tendência" class="level3">
<h3 class="anchored" data-anchor-id="modelando-a-tendência">Modelando a tendência:</h3>
<p>
É bem nítido que há uma tendência na série. Isso é, intrisicamente no Processo Estocástico (PE) gerador da série há alguma função do tempo que, sistematicamente, soma algum incremento na série, multiplique ela por um fator &gt; 1 ou faça ambos ao mesmo tempo.
</p>
<div class="cell" data-execution_count="12">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-13-output-1.png" width="677" height="434"></p>
</div>
</div>
<p><br></p>
<p>
Pra uma questão de facilidade nos cálculos, usar-se-á uma aproximação linear dessa possível função. O modelo da demanda no caso da regressão linear será:
</p>
<p><span class="math display">
  
  \[y_t = \beta_{0} + \beta_{1} t \]

</span></p>
<p>
</p><p>A interpretação dos parâmetros <span class="math inline">\(\beta_{0}\)</span> e <span class="math inline">\(\beta_{1}\)</span> é bem intuitiva. Como o input é o ano, o <span class="math inline">\(\beta_{1}\)</span> é a variação da demanda ano a ano. No caso, não tem como saber exatamente como será o modelo do XGBoost e a vantagem dessa classe de modelos é exatamente essa. Por se tratar de uma regressão não paramétrica, ele encontrará, nos dados, a melhor forma de descrever essa tendência em função do tempo.</p>
Criando uma função para automatizar o processo de data featuring para a tendência:
<p></p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_featuring_trend(df):</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.copy()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    X.loc[:, <span class="st">'year'</span>] <span class="op">=</span> [i.year <span class="cf">for</span> i <span class="kw">in</span> X.index]</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X.drop(labels<span class="op">=</span>[<span class="st">'beer'</span>],</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>                   axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>
Criando o modelo para os 21 backtest:
</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>cv_generator <span class="op">=</span> cv.split(df)</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(cv.split(df)))</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>acc_reg_1 <span class="op">=</span> []</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>acc_xgb_1 <span class="op">=</span> []</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> <span class="bu">next</span>(cv_generator)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Separando teste do treino</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> df.iloc[index[<span class="dv">0</span>]].copy()</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    X_train <span class="op">=</span> data_featuring_trend(y_train)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> df.iloc[index[<span class="dv">1</span>]].copy()</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    X_test <span class="op">=</span> data_featuring_trend(y_test)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Treinando modelos</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>    reg <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    reg.fit(X_train, y_train)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>    y_reg_pred <span class="op">=</span> reg.predict(X<span class="op">=</span>X_test)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    xgb <span class="op">=</span> XGBRegressor()</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    xgb.fit(X_train, y_train)</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    y_xgb_pred <span class="op">=</span> xgb.predict(X<span class="op">=</span>X_test)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Salvando acurácia dos modelos</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>    acc_reg_1.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.beer.values.<span class="bu">sum</span>()<span class="op">-</span> y_reg_pred.<span class="bu">sum</span>())<span class="op">/</span>y_test.beer.values.<span class="bu">sum</span>())</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    acc_xgb_1.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.beer.values.<span class="bu">sum</span>()<span class="op">-</span> y_xgb_pred.<span class="bu">sum</span>())<span class="op">/</span>y_test.beer.values.<span class="bu">sum</span>())</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>acc_reg_1,acc_xgb_1 <span class="op">=</span> np.array(acc_reg_1), np.array(acc_xgb_1)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_reg_1)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_xgb_1)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost vence: </span><span class="sc">{</span>(acc_xgb_1<span class="op">&gt;</span>acc_reg_1)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_1<span class="op">&gt;</span>acc_reg_1).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"regressão vence: </span><span class="sc">{</span>(acc_xgb_1<span class="op">&lt;</span>acc_reg_1)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_1<span class="op">&lt;</span>acc_reg_1).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.94956268 0.96975878 0.95273909 0.9895277  0.96724426 0.95933134
 0.996359   0.98306074 0.94584999 0.95548503 0.9947364  0.96824616
 0.97035176 0.97464616 0.98857959 0.97895506 0.97236466 0.97753643
 0.97691234 0.9985189  0.99684608]

[0.94874313 0.95867934 0.92875407 0.96239128 0.99086754 0.98112817
 0.94656535 0.95894754 0.93200233 0.94460743 0.96516789 0.98158768
 0.97207787 0.97662276 0.95552783 0.96286553 0.95423104 0.95521251
 0.94920657 0.96978884 0.95652298]

XGBoost vence: 5 (23.81 %)

regressão vence: 16 (76.19 %)</code></pre>
</div>
</div>
<p><br></p>
<p>
Como pode ser visto, o XGBoost perfoma acima da regressão linear em somente 23.81% dos casos. Será que a regressão realmente performa acima ou esse valor pode ter vindo do acaso? Bom, testa-se a hipótese de que a regressão linear tem probabilidade 50% de vencer o XGBoost.
</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">1</span>, <span class="bu">len</span>(acc_xgb_1) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>ax.plot(x, acc_xgb_1, <span class="st">'bo-'</span>, label<span class="op">=</span><span class="st">'XGBoost'</span>)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>ax.plot(x, acc_reg_1, <span class="st">'go-'</span>, label<span class="op">=</span><span class="st">'regressão'</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>ax.axhline(y<span class="op">=</span><span class="dv">1</span>, color<span class="op">=</span><span class="st">'k'</span>, ls<span class="op">=</span><span class="st">'--'</span>, alpha<span class="op">=</span><span class="fl">.3</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"Rodada"</span>)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"Acurácia"</span>)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="fl">.8</span>, <span class="fl">1.02</span>)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>ax.set_xticks([<span class="bu">int</span>(i) <span class="cf">for</span> i <span class="kw">in</span> ax.get_xticks()])</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>plt.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Análise de Performance"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-16-output-1.png" width="691" height="455"></p>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>test(a <span class="op">=</span> acc_reg_1,</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>     b <span class="op">=</span> acc_xgb_1)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Testes: 21
A venceu: 16
B venceu: 5

Há evidências para dizer que o A é o melhor modelo
p valor:  0.013</code></pre>
</div>
</div>
<p>
</p><p>Hipótese rejeitada. Isso significa que, sim, pode-se afirmar que a regressão produz uma acurácia maior que o XGBoost. A probabilidade dela não ser maior e ser observada esse placar ou um mais raro é somente 1.6%.</p>
<p>O ponto agora é discutir a natureza desses resultados. Afinal, o que a regressão linear entendeu que o XgBoost não entendeu? O que pode explicar esse melhor desempenho do modelo mais simples?</p>
Para responder essa questão, o próximo passo é explorar pelo menos o último modelo treinado e entender o que aconteceu. Fazendo um gráfico de dispersão das observações da série em termos das variáveis de entrada (somente o ano):
<p></p>
<div class="cell" data-execution_count="17">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-18-output-1.png" width="758" height="757"></p>
</div>
</div>
<p><br></p>
<p>
</p><p>O primeiro ponto é quanto a previsão “linear”. Sim, naturalmente a previsão vai ser uma reta horizontal. Dado que em todos os 12 pontos do ano há o mesmo input (o ano corrente).</p>
<p>O segundo ponto é a fraquíssima capacidade do XGboost em extrapolar a previsão para fora do espaço de treino das covariáveis. Bom, isso não é exatamente um problema exclusivo do XGBoost. Explica-se.</p>
<p>Na verdade, regressões, no geral, sem qualquer exceção, têm esse problema de extrapolação do espaço das covariáveis. A regressão linear também sofre com isso. Inclusive, quando se aprende regressão linear, aprende-se que deve-se evitar essa extrapolação. Não a toa, ela não venceu em todas, mas na maioria. E, nesse caso, performou melhor por ser mais simples. É um modelo que apenas estima a varição ano a ano média.</p>
<p>Analisando as nuances do teste, o que foi feito, em termos práticos, foi ensinar um modelo o padrão da venda de cerveja entre os inputs de 2012 a 2017. No entanto, o que foi exigido foi prever como vai ser essa venda em 2018. Parece até sacanagem com o coitado. De fato, ele entendeu a demanda entre 2012 e 2017, mas a falta de observações do futuro não permitiu ele entender o seu comportamento.</p>
<p>Entendido o porquê o XGBoost performou tão mal, fica, portanto, entendido que ele deve ser descartado, posto que em todos os problemas de séries temporais ter-se-á essa necessidade de extrapolação? A resposta é não. Tem como contornar esse problema, inclusive, aplicando metodologias inspiradas nos modelos clássicos de séries temporais, como remoção de tendência diferenciando a série.</p>
Por hora, fica no radar essa eventual limitação no que tange ao intervalo de predição.
<p></p>
<p><br></p>
</section>
<section id="modelando-a-sazonalidade" class="level3">
<h3 class="anchored" data-anchor-id="modelando-a-sazonalidade">Modelando a Sazonalidade:</h3>
<p>
A série tem um claro fator sazonal que pode ser observado na vizualização abaixo:
</p>
<div class="cell" data-execution_count="18">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-19-output-1.png" width="658" height="434"></p>
</div>
</div>
<p><br></p>
<p>
</p><p>E, agora, que já há uma previsão da tendência, o próximo passo é saber o quanto a demanda vai variar em cada mês em relação ao “nível” médio anual. Dessa forma, o modelo de regressão linear ganha essa cara:</p>
<p><span class="math display">\[y_t = \beta_{0} + \beta_{1} t + \beta_{jan} x_{jan} + \beta_{fev} x_{fev} + ... + \beta_{dez} x_{dez}\]</span></p>
<p>Onde as variáveis <span class="math inline">\(x_{k}\)</span> são variáveis binárias que correspondem ao mês. Isso é, <span class="math inline">\(x_{jan}\)</span> vale 1 se o mês for janeiro, 0 caso contrário. E, portanto, <span class="math inline">\(\beta_{jan}\)</span> é o “prêmio” que se soma a demanda por ser janeiro, podendo ser positivo ou não. O input da regressão linear DEVE ser feito via variáveis dummies, dado que o mês é uma variável qualitativa nominal e o modelo é linear.</p>
<p>Novamente, não é possível saber como o XGBoost vai ser interpretado, dado que trata-se de uma regressão não paramétrica. O input da sazonalidade pode ser dado como se o mês fosse uma variável numérica, dado que o modelo é baseado em árvores de decisão e conseguiria “captar” o movimento da sazonalidade.</p>
Criando funções que adicionam variáveis de sazonalidade:
<p></p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> data_featuring_sazo(df: pd.DataFrame, model: <span class="bu">str</span>) <span class="op">-&gt;</span> pd.DataFrame:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df.copy()</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    X.loc[:, <span class="st">'year'</span>] <span class="op">=</span> [i.year <span class="cf">for</span> i <span class="kw">in</span> X.index]</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    X.loc[:, <span class="st">'month'</span>] <span class="op">=</span> [i.month <span class="cf">for</span> i <span class="kw">in</span> X.index]</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> model <span class="op">==</span> <span class="st">'xgb'</span>:</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X.drop(labels<span class="op">=</span>[<span class="st">'beer'</span>],</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>                    axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        X_sazo <span class="op">=</span> pd.get_dummies(X[<span class="st">'month'</span>])</span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a>        X <span class="op">=</span> pd.concat([X, X_sazo], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X.drop(labels<span class="op">=</span>[<span class="st">'month'</span>, <span class="st">'beer'</span>],</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>                    axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Vizualizando quais seriam os inputs:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"XGBoost:"</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_featuring_sazo(df.iloc[<span class="op">-</span><span class="dv">12</span>:], <span class="st">'xgb'</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>))</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regressão:"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(data_featuring_sazo(df.iloc[<span class="op">-</span><span class="dv">12</span>:], <span class="st">'reg'</span>).reset_index(drop<span class="op">=</span><span class="va">True</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>XGBoost:

    year  month
0   2018      1
1   2018      2
2   2018      3
3   2018      4
4   2018      5
5   2018      6
6   2018      7
7   2018      8
8   2018      9
9   2018     10
10  2018     11
11  2018     12

Regressão:

    year  1  2  3  4  5  6  7  8  9  10  11  12
0   2018  1  0  0  0  0  0  0  0  0   0   0   0
1   2018  0  1  0  0  0  0  0  0  0   0   0   0
2   2018  0  0  1  0  0  0  0  0  0   0   0   0
3   2018  0  0  0  1  0  0  0  0  0   0   0   0
4   2018  0  0  0  0  1  0  0  0  0   0   0   0
5   2018  0  0  0  0  0  1  0  0  0   0   0   0
6   2018  0  0  0  0  0  0  1  0  0   0   0   0
7   2018  0  0  0  0  0  0  0  1  0   0   0   0
8   2018  0  0  0  0  0  0  0  0  1   0   0   0
9   2018  0  0  0  0  0  0  0  0  0   1   0   0
10  2018  0  0  0  0  0  0  0  0  0   0   1   0
11  2018  0  0  0  0  0  0  0  0  0   0   0   1</code></pre>
</div>
</div>
<p>Criando o modelo para os 21 backtest:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>cv_generator <span class="op">=</span> cv.split(df)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(cv.split(df)))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>acc_reg_2 <span class="op">=</span> []</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>acc_xgb_2 <span class="op">=</span> []</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> <span class="bu">next</span>(cv_generator)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Separando teste do treino</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> df.iloc[index[<span class="dv">0</span>]].copy()</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    X_train_xgb <span class="op">=</span> data_featuring_sazo(y_train, <span class="st">'xgb'</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    X_train_reg <span class="op">=</span> data_featuring_sazo(y_train, <span class="st">'reg'</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> df.iloc[index[<span class="dv">1</span>]].copy()</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    X_test_xgb <span class="op">=</span> data_featuring_sazo(y_test, <span class="st">'xgb'</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    X_test_reg <span class="op">=</span> data_featuring_sazo(y_test, <span class="st">'reg'</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Treinando modelos</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    reg <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    reg.fit(X_train_reg, y_train)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    y_reg_pred <span class="op">=</span> reg.predict(X<span class="op">=</span>X_test_reg)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    xgb <span class="op">=</span> XGBRegressor()</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    xgb.fit(X_train_xgb, y_train)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    y_xgb_pred <span class="op">=</span> xgb.predict(X<span class="op">=</span>X_test_xgb)</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Salvando acurácia dos modelos</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a>    acc_reg_2.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.beer.values.<span class="bu">sum</span>()<span class="op">-</span> y_reg_pred.<span class="bu">sum</span>())<span class="op">/</span>y_test.beer.values.<span class="bu">sum</span>())</span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>    acc_xgb_2.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.beer.values.<span class="bu">sum</span>()<span class="op">-</span> y_xgb_pred.<span class="bu">sum</span>())<span class="op">/</span>y_test.beer.values.<span class="bu">sum</span>())</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>acc_reg_2,acc_xgb_2 <span class="op">=</span> np.array(acc_reg_2), np.array(acc_xgb_2)</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_reg_2)</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_xgb_2)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost vence: </span><span class="sc">{</span>(acc_xgb_2<span class="op">&gt;</span>acc_reg_2)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_2<span class="op">&gt;</span>acc_reg_2).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"regressão vence: </span><span class="sc">{</span>(acc_xgb_2<span class="op">&lt;</span>acc_reg_2)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_2<span class="op">&lt;</span>acc_reg_2).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.94956268 0.96975878 0.95273909 0.9895277  0.96724426 0.95933134
 0.996359   0.98306074 0.94584999 0.95548503 0.9947364  0.96824616
 0.97035176 0.97464616 0.98857959 0.97895506 0.97236466 0.97753643
 0.97691234 0.9985189  0.99684608]

[0.9487429  0.95867867 0.92875393 0.96239207 0.99086747 0.98112894
 0.94656517 0.95894766 0.93200407 0.94460672 0.96516848 0.98158739
 0.97207693 0.97662203 0.95552704 0.96286434 0.95423209 0.9552115
 0.94920716 0.96978955 0.95652373]

XGBoost vence: 5 (23.81 %)

regressão vence: 16 (76.19 %)</code></pre>
</div>
</div>
<p><br></p>
<p>
E o XGBoost segue performando ABAIXO da regressão linear. Dando uma explorada na forma com que ele lidou com os dados:
</p>
<div class="cell" data-execution_count="22">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-23-output-1.png" width="658" height="361"></p>
</div>
</div>
<p>
</p><p>Dessa vez, a previsão tá com mais carinha de série temporal. No caso, em termos práticos, cada efeito sazonal estimado somou um fator (positivo ou negativo) naquele nível inicialmente estimado para a regressão linear.</p>
<p>O XGBoost nadou de braçada em treino, sequer é possível ver a linha dos dados em treino. No entanto, em teste ele subestimou a demanda. E isso se deve, novamente ao problema da extrapolação no espaço da covariáveis <i>ano</i>.</p>
Analizando previsão sob as covariáveis:
<p></p>
<ul>
<li>Nas covariáveis de sazonalidade:</li>
</ul>
<div class="cell" data-execution_count="23">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-24-output-1.png" width="658" height="434"></p>
</div>
</div>
<p><br></p>
<ul>
<li>Nas covariáveis de tendência:</li>
</ul>
<div class="cell" data-execution_count="24">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-25-output-1.png" width="658" height="434"></p>
</div>
</div>
<p><br></p>
<p>
</p><p>É percepitível que o XGBoost pegou bem o padrão sazonal, no entanto, erra em acertar o nível (tendência).</p>
A pergunta do milhão é: como contornar esse problema? E a resposta vem lá dos modelos clássicos da estatística. A remoção de tendência pode ser feita diferenciando a série.
<p></p>
</section>
<section id="removendo-tendência" class="level3">
<h3 class="anchored" data-anchor-id="removendo-tendência">Removendo Tendência:</h3>
<p>Agora será feito uma transformação na variável <span class="math inline">\(y_t\)</span> a fim de que a previsão dela seja “facilitada” para os modelos.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>ax.plot(df.diff().dropna()[<span class="st">'beer'</span>], <span class="st">'k'</span>)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Variação Nominal"</span>)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-26-output-1.png" width="681" height="434"></p>
</div>
</div>
<p><br></p>
<p>
A série continua, ainda, não estacionária, posto que sua variância aumenta em função do tempo. A gente pode remover essa heterocedasticidade usando o logaritmo da série (e o derivando, dado que ele também não é estacionário):
</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>f, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>ax.plot((np.log(df[<span class="st">'beer'</span>])).diff().dropna(), <span class="st">'k'</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Variação Nominal do Logaritmo"</span>)</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-27-output-1.png" width="668" height="435"></p>
</div>
</div>
<p><br></p>
<p>Neste caso, a série passa a se comportar de forma estacionária. Rodando os modelos novamente:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>cv_generator <span class="op">=</span> cv.split(df)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(<span class="bu">list</span>(cv.split(df)))</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>acc_reg_3 <span class="op">=</span> []</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>acc_xgb_3 <span class="op">=</span> []</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n):</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> <span class="bu">next</span>(cv_generator)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Separando teste do treino</span></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    y_train <span class="op">=</span> df.iloc[index[<span class="dv">0</span>]].copy()</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> df.iloc[index[<span class="dv">1</span>]].copy()</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Fazendo a transformação</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    df_temp <span class="op">=</span> np.log(pd.concat([y_train, y_test])).diff().dropna()</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>    y_train, y_test <span class="op">=</span> df_temp.loc[y_train.index[<span class="dv">1</span>:]], df_temp.loc[y_test.index]</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    X_train_xgb <span class="op">=</span> data_featuring_sazo(y_train, <span class="st">'xgb'</span>)</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    X_train_reg <span class="op">=</span> data_featuring_sazo(y_train, <span class="st">'reg'</span>)</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    X_test_xgb <span class="op">=</span> data_featuring_sazo(y_test, <span class="st">'xgb'</span>)</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    X_test_reg <span class="op">=</span> data_featuring_sazo(y_test, <span class="st">'reg'</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Treinando modelos</span></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>    reg <span class="op">=</span> LinearRegression(fit_intercept<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>    reg.fit(X_train_reg, y_train)</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    y_reg_pred <span class="op">=</span> reg.predict(X<span class="op">=</span>X_test_reg)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>    xgb <span class="op">=</span> XGBRegressor()</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>    xgb.fit(X_train_xgb, y_train)</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>    y_xgb_pred <span class="op">=</span> xgb.predict(X<span class="op">=</span>X_test_xgb)</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Retornando as variáveis para escala padrão (integrando):</span></span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.array([<span class="dv">0</span>] <span class="op">+</span> <span class="bu">list</span>(y_train.beer.values) <span class="op">+</span> <span class="bu">list</span>(y_test.beer.values))</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    y_test <span class="op">=</span> np.exp(np.log(df.iloc[index[<span class="dv">0</span>][<span class="dv">0</span>], <span class="dv">0</span>]) <span class="op">+</span> y_test.cumsum())[<span class="op">-</span><span class="dv">12</span>:]</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    y_reg_pred <span class="op">=</span> np.array([<span class="dv">0</span>] <span class="op">+</span> <span class="bu">list</span>(reg.predict(X<span class="op">=</span>X_train_reg)[:,<span class="dv">0</span>]) <span class="op">+</span> <span class="bu">list</span>(y_reg_pred[:,<span class="dv">0</span>]))</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    y_reg_pred <span class="op">=</span> np.exp(np.log(df.iloc[index[<span class="dv">0</span>][<span class="dv">0</span>], <span class="dv">0</span>]) <span class="op">+</span> y_reg_pred.cumsum())</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>    y_xgb_pred <span class="op">=</span> np.array([<span class="dv">0</span>] <span class="op">+</span> <span class="bu">list</span>(xgb.predict(X<span class="op">=</span>X_train_xgb)) <span class="op">+</span> <span class="bu">list</span>(y_xgb_pred))</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>    y_xgb_pred <span class="op">=</span> np.exp(np.log(df.iloc[index[<span class="dv">0</span>][<span class="dv">0</span>], <span class="dv">0</span>]) <span class="op">+</span> y_xgb_pred.cumsum())</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Salvando acurácia dos modelos</span></span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>    acc_reg_3.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.<span class="bu">sum</span>() <span class="op">-</span> y_reg_pred[<span class="op">-</span><span class="dv">12</span>:].<span class="bu">sum</span>())<span class="op">/</span>y_test.<span class="bu">sum</span>())</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>    acc_xgb_3.append(<span class="dv">1</span><span class="op">-</span><span class="bu">abs</span>(y_test.<span class="bu">sum</span>()<span class="op">-</span> y_xgb_pred[<span class="op">-</span><span class="dv">12</span>:].<span class="bu">sum</span>())<span class="op">/</span>y_test.<span class="bu">sum</span>())</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a>acc_reg_3,acc_xgb_3 <span class="op">=</span> np.array(acc_reg_3), np.array(acc_xgb_3)</span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_reg_3)</span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(acc_xgb_3)</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"XGBoost vence: </span><span class="sc">{</span>(acc_xgb_3<span class="op">&gt;</span>acc_reg_3)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_3<span class="op">&gt;</span>acc_reg_3).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"regressão vence: </span><span class="sc">{</span>(acc_xgb_3<span class="op">&lt;</span>acc_reg_3)<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">}</span><span class="ss"> (</span><span class="sc">{</span><span class="bu">round</span>(<span class="dv">100</span><span class="op">*</span>(acc_xgb_3<span class="op">&lt;</span>acc_reg_3).<span class="bu">sum</span>()<span class="op">/</span>n, <span class="dv">2</span>)<span class="sc">}</span><span class="ss"> %)"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.99532842 0.93930839 0.95202871 0.97169372 0.93077445 0.98235854
 0.97826689 0.97537997 0.98561417 0.99258848 0.98632184 0.96603365
 0.98250614 0.95616816 0.98036951 0.99492786 0.97460849 0.98595941
 0.99981699 0.99789177 0.98427603]

[0.94957249 0.96914387 0.99811368 0.99475338 0.96131824 0.96691168
 0.98988474 0.99735181 0.9850433  0.9733967  0.9965312  0.99662964
 0.99226722 0.98902899 0.99856802 0.99026485 0.97094775 0.98083103
 0.99213042 0.98982775 0.98768803]

XGBoost vence: 12 (57.14 %)

regressão vence: 9 (42.86 %)</code></pre>
</div>
</div>
<p><br></p>
<p>
E o jogo virou! O XGBoost performa melhor 12 vezes, contra 9 da regressão linear. Será que vencer 57% de 21 duelos é o bastante para dizer que ele é melhor? Bom, é uma hipótese que pode ser testada.
</p>
<div class="cell" data-execution_count="28">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-29-output-1.png" width="691" height="455"></p>
</div>
</div>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>test(a<span class="op">=</span>acc_xgb_3,</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>     b<span class="op">=</span>acc_reg_3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Testes: 21
A venceu: 12
B venceu: 9

Não há evidências para dizer que o A é o melhor modelo
p valor:  0.332</code></pre>
</div>
</div>
<p>
</p><p>Observa-se um p-valor de 33%. Isso é, não possível rejeitar a hipótese de que os dois modelos têm a mesma performance e afirmar que a probabilidade do XGBoost ser melhor que o outro é maior que 50% (o acaso) ainda.</p>
Outra questão a ser levanta é se essa transformação de fato gerou maior performance na predição. Afinal, não é o objetivo encontrar um modelo que preveja a variação do logaritmo da série, mas, sim, a própria série. Para isso, novamente, testa-se a hipótese:
<p></p>
<div class="cell" data-execution_count="30">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-31-output-1.png" width="691" height="455"></p>
</div>
</div>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>test(a<span class="op">=</span>acc_xgb_3,</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>     b<span class="op">=</span>acc_reg_2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Testes: 21
A venceu: 15
B venceu: 6

Há evidências para dizer que o A é o melhor modelo
p valor:  0.039</code></pre>
</div>
</div>
<p>
</p><p>Resultado interessante. Não se pode afirmar que o XGBoost é melhor do que a regressão, mas pode-se afirmar que a previsão da variável transformada retorna valores maiores de acurácia do que ela em nível.</p>
Isso acontece porque a transformação logaritmica garante um controle da heterocedasticidade na sazonalidade. Nela o <span class="math inline">\(\beta_{jan}\)</span> é um fator multiplicativo e não um “prêmio”.
<p></p>
Bom, dito isso, agora é hora de explorar a nova cara do XGBoost usando o último modelo:
<p></p>
<ul>
<li>Vizualizando em nível:</li>
</ul>
<div class="cell" data-execution_count="32">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-33-output-1.png" width="658" height="435"></p>
</div>
</div>
<p><br></p>
<ul>
<li>Nas covariáveis de sazonalidade:</li>
</ul>
<div class="cell" data-execution_count="33">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-34-output-1.png" width="658" height="434"></p>
</div>
</div>
<ul>
<li>Nas covariáveis de tendência:</li>
</ul>
<div class="cell" data-execution_count="34">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-35-output-1.png" width="658" height="434"></p>
</div>
</div>
<p><br></p>
<p>
</p><p>Como pode ser visto, agora o modelo XGBoost tem a capacidade de entender tendências e heterocedasticidade!! O que mudou? O problema de extrapolação nas covariáveis desapareceu? A resposta é não.</p>
A única variável extrapolada é a variável <i>ano</i> e inicialmente ela tinha uma tendência. Após remover essa tendência, ela passou a ser estacionária, como pode ser visto abaixo:
<p></p>
<div class="cell" data-execution_count="35">
<div class="cell-output cell-output-display">
<p><img src="qmd_file/figure-html/cell-36-output-1.png" width="656" height="434"></p>
</div>
</div>
<p><br></p>
<p>
Como pode ser visto, o que era pra ser previsto na variável <span class="math inline">\(y\)</span> era <span class="math inline">\(\log{y_t} - \log{y_{t-1}}\)</span>, que é uma variável estacionária e, portanto, a extrapolação não exige o conhecimento de nenhuma tendência. O problema ainda existe, mas é suavizado.
</p>
</section>
</section>
<section id="comentários-finais" class="level2">
<h2 class="anchored" data-anchor-id="comentários-finais">Comentários Finais:</h2>
<p>
</p><p>Recapitulando o que foi visto:</p>
<ul>
<li>Definiu-se o que são regressões não paramétricas</li>
<li>Foi apresentada uma classe de regressões não paramétricas, o Gradient Boost Tree Decision</li>
<li>Foi explorado os problemas dessa aborgadem na previsão de demanda usando o XGBoost</li>
<li>Foi apresentada possíveis soluções</li>
</ul>
<p>Lembrando, foi usado o XGBoost, mas poderia ser usado o LightLGB, o KNN, Splines etc. O ponto central está no entendimento da aplicação desses modelos.</p>
<p>Outro ponto a se destacar foi a importância da remoção da tendência. Apesar de ser um modelo “moderninho” de machine learning, o que ajudou ele a performar melhor foi um approach mais clássico de séries temporais. No final, não importa o modelo, aplicar uma metodologia baseada em Box &amp; Jenkis nunca faz mal. Transformações nos dados também ajudam muito.</p>
<p>No mais, trata-se de um modelo promissor para dados estacionários com forte padrão de sazonal. É muito usado quando se trata de uma granulalidade alta, seja no tempo, seja na previsão (sku, pdv etc), dado que ele entende fácil padrões estruturais.</p>
Além disso, usa-se muito variáveis com lag, afim de dar uma cara mais de “SARMA” pra ele. Novamente a ideia de usar do que há mais bem conceituada na estatística clássica, mas com modelos não lineares mais robustos.
<p></p>
</section>
<section id="principais-fontes" class="level2">
<h2 class="anchored" data-anchor-id="principais-fontes">Principais Fontes:</h2>
<p>
</p><ul>
<li><p>Para regressões não paramétricas, há as notas de aula do professor Lucambio Perez da UFPR: <a href="http://leg.ufpr.br/~lucambio/Nonparam/NparamIV.html">Regressão Não Paramétrica</a>.</p></li>
<li><p>Para uma boa introdução ao universo de séries temporais e previsão, o livro <a href="https://otexts.com/fpp3/">Forecasting: Principles &amp; Practice</a>. O livro é todo construído em R, o que pode ser um problema para os pythonistas. Os autores têm excelentes publicações na área de séries temporais, que são devidamente citadas ao longo do livro, então é uma baita livro de cabeceira.</p></li>
<li><p>Para sofrer um pouquinho com séries temporais e entender a fundo a “lógica” da modelagem, o livro <a href="https://www.amazon.com.br/Econometria-S%C3%A9ries-Temporais-Rodrigo-Silveira/dp/852211157X">Econometria de Séries Temporais</a>. Não recomendo como primeira leitura, mas certamente um baita livro para elevar o nível de entendimento no assunto.</p></li>
<li><p>Para uma introdução no universo do Machine Learning, o livro <a href="https://www.amazon.com.br/M%C3%A3os-obra-aprendizado-scikit-learn-tensorflow/dp/8550803812/ref=asc_df_8550803812/?tag=googleshopp00-20&amp;linkCode=df0&amp;hvadid=379748659420&amp;hvpos=&amp;hvnetw=g&amp;hvrand=6197706445306642602&amp;hvpone=&amp;hvptwo=&amp;hvqmt=&amp;hvdev=c&amp;hvdvcmdl=&amp;hvlocint=&amp;hvlocphy=1001634&amp;hvtargid=pla-812887614657&amp;psc=1">Mãos à Obra: Aprendizado de Máquina com Scikit-Learn &amp; TensorFlow</a>. É um livro básico, mas que é legal ter como consulta, como foi o caso na construção do texto.</p></li>
<li><p>Para mais usos de Aprendizado de Máquina Supervisionado em séries temporais, o livro <a href="https://www.amazon.com/Advanced-Forecasting-Python-State-Art-Models/dp/1484271491">Advanced Forecasting with Python</a>. Não é tão <i>advanced</i> assim e o autor acaba pecando fortemente na exploração de alguns modelos, mas é o preço que se paga por querer ensinar +10 modelos distintos em menos de 300 pgs.</p></li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>